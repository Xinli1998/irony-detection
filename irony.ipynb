{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV4HOYpMaxTv",
        "outputId": "885b9a38-b7cc-4fe4-bb21-418ddb3aacad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(\"/content/drive/MyDrive/nlp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_YUiskSU-eC"
      },
      "outputs": [],
      "source": [
        "sys.path.append(\"/content/drive/MyDrive/lp\")\n",
        "import vocab_helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGgWUi9Ka53O"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "import numpy as np\n",
        "import glob\n",
        "import xml.etree.cElementTree as ET\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2zN2XbUa8AM"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n",
        "import emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-fNBobHbOk_"
      },
      "outputs": [],
      "source": [
        "GT    = '/content/drive/MyDrive/nlp/truth.txt'\n",
        "true_values = {}\n",
        "f=open(GT)\n",
        "for line in f:\n",
        "    linev = line.strip().split(\":::\")\n",
        "    true_values[linev[0]] = linev[1]\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDH8QJ604LRV"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55vxI8W447EZ"
      },
      "outputs": [],
      "source": [
        "def replace_contracted_form(contracted_word, pos, dictionary):\n",
        "    long_form = []\n",
        "    if \"'\" in contracted_word:\n",
        "        # print(\"Found apostrophe in word: \", contracted_word, ' with pos: ', pos)\n",
        "        split_words = contracted_word.split(\"'\")\n",
        "        check_if_in_dict = False\n",
        "        # If the contraction is a nominal + verbal or a proper noun + verbal\n",
        "        if pos is 'L' or pos is 'M':\n",
        "            long_form.append(split_words[0])\n",
        "            if split_words[1].lower() in contractions:\n",
        "                long_form.extend(contractions[split_words[1].lower()].split())\n",
        "        # If the contraction is a whole verb (like let's or isn't)\n",
        "        elif pos in ['V', 'Y', 'O'] and contracted_word.lower() in contractions:\n",
        "            long_form.extend(contractions[contracted_word.lower()].split())\n",
        "        # If the contraction is proper noun with possessive or a nominal with a possessive or even a (proper) noun\n",
        "        elif pos in ['S', 'Z', 'D', 'N', '^']:\n",
        "            if contracted_word.lower() in contractions:\n",
        "                long_form.extend(contractions[contracted_word.lower()].split())\n",
        "            elif split_words[1].lower() == 's':\n",
        "                long_form.append(split_words[0])\n",
        "            elif contracted_word.lower() in contractions:\n",
        "                long_form.extend(contractions[contracted_word.lower()].split())\n",
        "            else:\n",
        "                check_if_in_dict = True\n",
        "        # Can skip ' which are just punctuation marks (usually used to emphasize or quote something)\n",
        "        elif pos is ',':\n",
        "            # print(\"Punctuation, nothing to replace.\", split_words[0], ' -- ', split_words[1])\n",
        "            return []\n",
        "        # Never replace contractions in emojis or emoticons (will be translated later)\n",
        "        elif pos is 'E':\n",
        "            long_form.append(contracted_word)\n",
        "        else:\n",
        "            check_if_in_dict = True\n",
        "        if check_if_in_dict:\n",
        "            # Attempt to separate words which have been separated by ' by human error\n",
        "            clean0 = re.findall(\"[a-zA-Z]+\", split_words[0])\n",
        "            clean1 = re.findall(\"[a-zA-Z]+\", split_words[1])\n",
        "            if clean0 != [] and clean0[0].lower() in dictionary and clean1 != [] and clean1[0].lower() in dictionary:\n",
        "                # print(\"Cleaned to \", clean0, ', ', clean1)\n",
        "                long_form.extend([clean0[0], clean1[0]])\n",
        "            else:\n",
        "                # print(\"Word couldn't be de-contracted!\")\n",
        "                long_form.append(contracted_word)\n",
        "        return long_form\n",
        "    else:\n",
        "        return long_form.append(contracted_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJhLf3ZNbRD2"
      },
      "outputs": [],
      "source": [
        "DIREC = \"/content/drive/MyDrive/nlp/\"\n",
        "\n",
        "# data cleaning\n",
        "text1=[]\n",
        "sent = []\n",
        "sent_y =[]\n",
        "y=[]\n",
        "for file in glob.glob(DIREC+\"*.xml\"):\n",
        "    tree = ET.parse(file)\n",
        "    doc = tree.iter(\"document\")\n",
        "    l= []\n",
        "    for d in doc:\n",
        "        c = d.text\n",
        "        # c = re.sub(u\"\\\\#.*?\\\\#\", \"\", c)\n",
        "        l_str = c.replace(\"USER\", \"\").replace(\"URL\", \"\")\n",
        "        l_str = l_str.strip()\n",
        "        # l_str = emoji.demojize(l_str)\n",
        "        sent.append(l_str)\n",
        "        l.append(l_str)\n",
        "    a = \" \".join(l)\n",
        "    number = len(l)\n",
        "    USERCODE = file.split(\"/\")[-1][:-4]\n",
        "    if true_values[USERCODE] =='NI':\n",
        "        y.append(0)\n",
        "        sent_y+=[0]*number\n",
        "    else:\n",
        "        y.append(1)\n",
        "        sent_y+=[1]*number\n",
        "\n",
        "    text1.append(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XP6QTk0mqOb"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOEoRgS6bTXn"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(list(text1),list(y),test_size = 0.2,shuffle = True,random_state=42)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XxdUjHuLt3UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFmwvCqg7tUC"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "!pip install contractions\n",
        "from nltk.stem import PorterStemmer\n",
        "import contractions\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stop = stopwords + list(string.punctuation)\n",
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stop]\n",
        "    return output\n",
        "def lemmatizer(text):\n",
        "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "    return lemm_text\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# init stemmer\n",
        "porter_stemmer=PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxbPcy_G6_ge"
      },
      "outputs": [],
      "source": [
        "def furtherclean(data):\n",
        "    data_clean = []\n",
        "    for text in data:\n",
        "      contract = contractions.fix(text)\n",
        "      token = word_tokenize(str(contract).lower())\n",
        "      nostop = remove_stopwords(token)\n",
        "      stemmed_words=[porter_stemmer.stem(word=word) for word in nostop]\n",
        "      data_clean.append(stemmed_words)\n",
        "    return data_clean  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DgHg4q8Bfcv"
      },
      "outputs": [],
      "source": [
        "clean_train = furtherclean(x_train)\n",
        "clean_test = furtherclean(x_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pmjToqmTuiH"
      },
      "outputs": [],
      "source": [
        "# len(clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRt1fdwsJCLT"
      },
      "outputs": [],
      "source": [
        "#\n",
        "def posfeature (data):\n",
        "\n",
        "    pos_tag = []\n",
        "    for text in data:\n",
        "        pos = nltk.pos_tag(text)\n",
        "        output = [tuple[1] for tuple in pos]\n",
        "        pos_tag.append(' '.join(output))\n",
        "\n",
        "    tf_pos = TfidfVectorizer(ngram_range=(1, 1), max_features=30, norm='l2') \n",
        "    tf_pos_feature = tf_pos.fit_transform(pos_tag)\n",
        "    tf_pos_feature = tf_pos_feature.toarray()\n",
        "    return tf_pos_feature"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x_kLIaCw79Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBf8y5DlJL35"
      },
      "outputs": [],
      "source": [
        "posfeature_train = posfeature(clean_train)\n",
        "posfeature_test = posfeature(clean_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXZaLNnSPX2W"
      },
      "outputs": [],
      "source": [
        "#semantic\n",
        "def tf_idffeature (data):\n",
        "    full = []\n",
        "    for text in data:\n",
        "        text = ' '.join(str(x) for x in text)\n",
        "        full.append(text)\n",
        "    # tfidf feature:\n",
        "        # character based:\n",
        "    tf_char = TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 3), max_features=32, norm='l2')\n",
        "    tf_char_feature = tf_char.fit_transform(full).toarray()\n",
        "        # word based\n",
        "    tf_word = TfidfVectorizer(ngram_range=(1, 3), max_features=50, norm='l2')\n",
        "    tf_word_feature = tf_word.fit_transform(full).toarray()\n",
        "\n",
        "    return tf_char_feature,tf_word_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVKmc6_-PuUT"
      },
      "outputs": [],
      "source": [
        "tf_char_train,tf_word_train = tf_idffeature(clean_train)\n",
        "tf_char_test,tf_word_test = tf_idffeature(clean_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P91xqV7EbZPe"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# path=\"/content/drive/MyDrive/nlp/vocab_helpers\"\n",
        "# os.chdir(path)\n",
        "# os.listdir(path)\n",
        "\n",
        "import emoji, re, time, os\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk import ngrams\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import MmCorpus, Dictionary\n",
        "import vocab_helpers as helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvzPVyaYZRBC"
      },
      "outputs": [],
      "source": [
        "def build_subj_dicionary(lines):\n",
        "    subj_dict = dict()\n",
        "    for line in lines:\n",
        "        splits = line.split(' ')\n",
        "        if len(splits) == 6:\n",
        "            word = splits[2][6:]        # the word analyzed\n",
        "            word_type = splits[0][5:]   # weak or strong subjective\n",
        "            pos = splits[3][5:]         # part of speech: noun, verb, adj, adv or anypos\n",
        "            polarity = splits[5][14:]   # its polarity: can be positive, negative or neutral\n",
        "            new_dict_entry = {pos: [word_type, polarity]}\n",
        "            if word in subj_dict.keys():\n",
        "                subj_dict[word].update(new_dict_entry)\n",
        "            else:\n",
        "                subj_dict[word] = new_dict_entry\n",
        "    return subj_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4ojwVUZbdSV"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "def normal_feature(data):\n",
        "  text_normal_feature=[]\n",
        "  for tweet_tokens in data:\n",
        "    tweet_tokens=word_tokenize(tweet_tokens)\n",
        "    capitalized_words = user_specific = intensifiers = tweet_len_ch = 0\n",
        "    for t in tweet_tokens:\n",
        "            tweet_len_ch += len(t)\n",
        "            if t.isupper() and len(t) > 1:\n",
        "                capitalized_words += 1       # count of capitalized words\n",
        "            if t.startswith(\"@\"):\n",
        "                user_specific += 1          # count of user mentions\n",
        "            if t.startswith(\"#\"):\n",
        "                user_specific += 1          # count-based feature of hashtags used (excluding sarcasm or sarcastic)\n",
        "            if t.lower().startswith(\"haha\") or re.match('l(o)+l$', t.lower()):\n",
        "                user_specific += 1          # binary feature marking the presence of laughter\n",
        "            if t in helper.strong_negations:\n",
        "                intensifiers += 1           # count-based feature of strong negations\n",
        "            if t in helper.strong_affirmatives:\n",
        "                intensifiers += 1           # count-based feature of strong affirmatives\n",
        "            if t in helper.interjections:\n",
        "                intensifiers += 1           # count-based feature of relevant interjections\n",
        "            if t in helper.intensifiers:\n",
        "                intensifiers += 1           # count-based feature of relevant intensifiers\n",
        "            if t in helper.punctuation:\n",
        "                user_specific += 1          # count-based feature of relevant punctuation signs\n",
        "            if t in emoji.UNICODE_EMOJI:\n",
        "                user_specific += 1          # count-based feature of emojis\n",
        "    tweet_len_tokens = len(tweet_tokens)  # get the length of the tweet in tokens\n",
        "    average_token_length = float(tweet_len_tokens) / max(1.0, float(tweet_len_ch))  # average tweet length\n",
        "    feature_list = {'tw_len_ch': tweet_len_ch, 'tw_len_tok': tweet_len_tokens, 'avg_len': average_token_length,\n",
        "                    'capitalized': capitalized_words, 'user_specific': user_specific, 'intensifiers': intensifiers}\n",
        "    text_normal_feature.append(feature_list)\n",
        "  return text_normal_feature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5TT5PI_9yEE"
      },
      "outputs": [],
      "source": [
        "f = open('/content/drive/MyDrive/nlp/emoji/emoji_sentiment_dictionary.txt')\n",
        "lines = f.readlines()      #读取全部内容 ，并以列表方式返回\n",
        "emojii={}\n",
        "for line in lines:\n",
        "  b=[]\n",
        "  l=line.split()\n",
        "  # a=word_tokenize(l[4:])\n",
        "  b.append(l[0])\n",
        "  b.append(float(l[1]))\n",
        "  b.append(float(l[2]))\n",
        "  b.append(float(l[2]))\n",
        "  b.append(\" \".join(l[4:]))\n",
        "  emojii[b[0]]=b[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WbBNNbAZZeK"
      },
      "outputs": [],
      "source": [
        "file = open('/content/drive/MyDrive/nlp/subjectivity_lexicon.tff','r')\n",
        "text = file.read()\n",
        "lexicon = text.split(\"\\n\")\n",
        "subj_dict = build_subj_dicionary(lexicon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVvrP-2-A5vC"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "def senti_feature(data,subj_dict,emojii):\n",
        "  text_senti_feature=[]\n",
        "\n",
        "  for tweet_token in data:\n",
        "    tweet_tokens=word_tokenize(tweet_token)\n",
        "  #\"positive emoji\", \"negative emoji\", \"neutral emoji\",\n",
        "    sent_features = dict.fromkeys([\"positive emoji\", \"negative emoji\", \"neutral emoji\",\n",
        "                                    \"subjlexicon weaksubj\", \"subjlexicon strongsubj\",\n",
        "                                    \"subjlexicon positive\", \"subjlexicon negative\",\n",
        "                                    \"subjlexicon neutral\", \"total sentiment words\",\n",
        "                                    \"swn pos\", \"swn neg\", \"swn obj\"], 0.0)\n",
        "    for t in tweet_tokens:\n",
        "    \n",
        "        if t in emojii.keys():\n",
        "            sent_features['negative emoji'] += float(emojii[t][0])\n",
        "            sent_features['neutral emoji'] += float(emojii[t][1])\n",
        "            sent_features['positive emoji'] += float(emojii[t][2])\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    pos_translation = {'N': 'noun', 'V': 'verb', 'D': 'adj', 'R': 'adverb'}\n",
        "    for index in range(len(tweet_tokens)):\n",
        "        lemmatized = lemmatizer.lemmatize(tweet_tokens[index], 'v')\n",
        "        if lemmatized in subj_dict.keys():\n",
        "            if tweet_tokens[index] in pos_translation and pos_translation[tweet_tokens[index]] in subj_dict[lemmatized].keys():\n",
        "                # Get the type of subjectivity (strong or weak) of this lemmatized word\n",
        "                sent_features['subjlexicon ' + subj_dict[lemmatized][pos_translation[tweet_tokens[index]]][0]] += 1\n",
        "                # Get the type of polarity (pos, neg, neutral) of this lemmatized word\n",
        "                if subj_dict[lemmatized][pos_translation[tweet_tokens[index]]][1] == 'both':\n",
        "                    sent_features['subjlexicon positive'] += 1\n",
        "                    sent_features['subjlexicon negative'] += 1\n",
        "                else:\n",
        "                    sent_features['subjlexicon ' + subj_dict[lemmatized][pos_translation[tweet_tokens[index]]][1]] += 1\n",
        "            else:\n",
        "                if 'anypos' in subj_dict[lemmatized].keys():\n",
        "                    # Get the type of subjectivity (strong or weak) of this lemmatized word\n",
        "                    sent_features['subjlexicon ' + subj_dict[lemmatized]['anypos'][0]] += 1 # strong or weak subjectivity\n",
        "                    # Get the type of polarity (pos, neg, neutral) of this lemmatized word\n",
        "                    if subj_dict[lemmatized]['anypos'][1] == 'both':\n",
        "                        sent_features['subjlexicon positive'] += 1\n",
        "                        sent_features['subjlexicon negative'] += 1\n",
        "                    else:\n",
        "                        sent_features['subjlexicon ' + subj_dict[lemmatized]['anypos'][1]] += 1\n",
        "\n",
        "    # Use the total number of sentiment words as a feature\n",
        "    sent_features[\"total sentiment words\"] = sent_features[\"subjlexicon positive\"] \\\n",
        "                                              + sent_features[\"subjlexicon negative\"] \\\n",
        "                                              + sent_features[\"subjlexicon neutral\"]\n",
        "    # Obtain average of all sentiment words (pos, ne, obj) using SentiWordNet Interface\n",
        "    pos_translation = {'N': 'n', 'V': 'v', 'D': 'a', 'R': 'r'}\n",
        "    for index in range(len(tweet_tokens)):\n",
        "        lemmatized = lemmatizer.lemmatize(tweet_tokens[index], 'v')\n",
        "        if tweet_tokens[index] in pos_translation:\n",
        "            synsets = list(swn.senti_synsets(lemmatized, pos_translation[tweet_tokens[index]]))\n",
        "            pos_score = 0\n",
        "            neg_score = 0\n",
        "            obj_score = 0\n",
        "            if len(synsets) > 0:\n",
        "                for syn in synsets:\n",
        "                    pos_score += syn.pos_score()\n",
        "                    neg_score += syn.neg_score()\n",
        "                    obj_score += syn.obj_score()\n",
        "                sent_features[\"swn pos\"] = pos_score / float(len(synsets))\n",
        "                sent_features[\"swn neg\"] = neg_score / float(len(synsets))\n",
        "                sent_features[\"swn obj\"] = obj_score / float(len(synsets))\n",
        "\n",
        "    # Vader Sentiment Analyser\n",
        "    # Obtain the negative, positive, neutral and compound scores of a tweet\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    polarity_scores = sia.polarity_scores(tweet_token)\n",
        "    for name, score in polarity_scores.items():\n",
        "        sent_features[\"Vader score \" + name] = score\n",
        "    text_senti_feature.append(sent_features)\n",
        "  return text_senti_feature"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w0FhJaNM_ZFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cfu9TOZH2uwj"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kRbtd39x0gK"
      },
      "outputs": [],
      "source": [
        "f = open('/content/drive/MyDrive/nlp/glove.6B.50d.txt')\n",
        "wordmap = {}\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    weights = np.asarray(values[1:], dtype='float32')\n",
        "    wordmap[word] = weights\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELJs-8uFKJJK"
      },
      "outputs": [],
      "source": [
        "def load_emb(data, wordmap, embedding_size=50, embedding_vocab=400000, init_emb=None):\n",
        "    # if init_emb is None:\n",
        "    #     emb_mat = np.random.randn(len(w2i), embedding_size)\n",
        "    # else:\n",
        "    #     emb_mat = init_emb\n",
        "    # f = open(path_to_embedding, 'r')\n",
        "    found = 0\n",
        "    em_list=[]\n",
        "    em_avg=[]\n",
        "    for text in data:\n",
        "      tweet_tokens=word_tokenize(text)\n",
        "      emb_mat=np.zeros([len(word_tokenize(text)), embedding_size])\n",
        "      for i in range(len(tweet_tokens)):\n",
        "        if tweet_tokens[i] in wordmap.keys():\n",
        "            emb_mat[i] = wordmap[tweet_tokens[i]]\n",
        "            # emb_mat[idx] = np.nan_to_num(vector)\n",
        "            found += 1\n",
        "      # for line in f:\n",
        "      #   content = line.strip().split()\n",
        "      #   vector = np.asarray(list(map(lambda x: float(np.nan_to_num(x)), content[-embedding_size:])))\n",
        "      #   print(vector.shape)\n",
        "      #   word = ' '.join(content[:-embedding_size])\n",
        "\n",
        "      em_list.append(emb_mat)\n",
        "      em_avg.append(np.mean(emb_mat,axis=0))\n",
        "    print(f\"Found {found} words in the embedding file.\")\n",
        "    return np.array(em_avg),em_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpoU6KuRqQBY"
      },
      "outputs": [],
      "source": [
        "# path_to_embedding='/content/drive/MyDrive/nlp/glove.6B.100d.txt'\n",
        "# em_avg,em_list=load_emb(text1, wordmap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0xIhoas8no_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w80vwpbjq9m_"
      },
      "outputs": [],
      "source": [
        "x_train_glove,x_train_glove_list=load_emb(x_train, wordmap)\n",
        "x_test_glove,x_test_glove_list=load_emb(x_test, wordmap)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l=[]\n",
        "for text in text1:\n",
        "  tweet_tokens=word_tokenize(text)\n",
        "  l.append(len(tweet_tokens))\n",
        "  # emb_mat=np.zeros([len(tweet_tokens), 50])\n",
        "  # for i in range(len(tweet_tokens)):\n",
        "  #   if tweet_tokens[i] in wordmap.keys():\n",
        "  #       emb_mat[i] = wordmap[tweet_tokens[i]]\n",
        "  #       # emb_mat[idx] = np.nan_to_num(vector)\n"
      ],
      "metadata": {
        "id": "oZUxWeKAAWr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_glove_1,x_train_glove_list_1=load_emb(x_train, wordmap)"
      ],
      "metadata": {
        "id": "-QRl7ERGBeqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_glove_1[-1].shape"
      ],
      "metadata": {
        "id": "K6hExVLXCtsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_glove_1[1]"
      ],
      "metadata": {
        "id": "vW2t_BFIBiO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsRDCO2Z0Dwg"
      },
      "outputs": [],
      "source": [
        "np.savetxt('/content/drive/MyDrive/nlp/x_train_glove.txt',x_train_glove)\n",
        "np.savetxt('/content/drive/MyDrive/nlp/x_test_glove.txt',x_test_glove)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_glove.shape"
      ],
      "metadata": {
        "id": "HLR9UHacTb-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdT8rw_8yIqs"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(u, v):\n",
        "    dot = np.dot(u, v)\n",
        "    norm_u = np.sqrt(np.sum(u ** 2))\n",
        "    norm_v = np.sqrt(np.sum(v ** 2))\n",
        "    cosine_distance = dot / (norm_u * norm_v)\n",
        "    return cosine_distance\n",
        "def euclidean_distance(u_vector, v_vector):\n",
        "    distance = np.sqrt(np.sum([(u - v) ** 2 for u, v in zip(u_vector, v_vector)]))\n",
        "    return distance\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U9lQ__Ez99e"
      },
      "outputs": [],
      "source": [
        "def get_similarity_measures(tweet, vec_map, weighted=False, verbose=False):\n",
        "    # Filter a bit the tweet so that no punctuation and no stopwords are included\n",
        "    # filtered_tweet = list(set([w.lower() for w in tweet.split()\n",
        "    #                            if w.isalnum() and w not in stopwords and w.lower() in vec_map.keys()]))\n",
        "    # Compute similarity scores between any 2 words in filtered tweet\n",
        "    similarity_scores_avg = []\n",
        "    max_words = []\n",
        "    min_words = []\n",
        "    for text in tweet:\n",
        "      similarity_dic = dict.fromkeys([\"most similar\", \"most dissimilar\"], 0.0)\n",
        "      tweet_tokens=word_tokenize(text)\n",
        "      similarity_scores = []\n",
        "      max_score = -100\n",
        "      min_score = 100\n",
        "      for i in range(len(tweet_tokens)-1):\n",
        "          wi = tweet_tokens[i]\n",
        "          for j in range(i + 1, len(tweet_tokens)):\n",
        "            try:\n",
        "                wj = text[j]\n",
        "                similarity = cosine_similarity(vec_map[wi], vec_map[wj])\n",
        "                if weighted:\n",
        "                    similarity /= euclidean_distance(vec_map[wi], vec_map[wj])\n",
        "                similarity_scores.append(similarity)\n",
        "                if max_score < similarity:\n",
        "                    max_score = similarity\n",
        "                    max_words = [wi, wj]\n",
        "                if min_score > similarity:\n",
        "                    min_score = similarity\n",
        "                    min_words = [wi, wj]\n",
        "            except: continue\n",
        "      max_score=max(similarity_scores)\n",
        "      min_score=min(similarity_scores)\n",
        "      similarity_dic['most similar']= max_score\n",
        "      similarity_dic['most dissimilar']=min_score\n",
        "      similarity_scores_avg.append(similarity_dic)\n",
        "      # if verbose:\n",
        "      #     print(\"Filtered tweet: \", text)\n",
        "      #     if max_score != -100:\n",
        "      #         print(\"Maximum similarity is \", max_score, \" between words \", max_words)\n",
        "      #     else:\n",
        "      #         print(\"No max! Scores are: \", similarity_scores)\n",
        "      #     if min_score != 100:\n",
        "      #         print(\"Minimum similarity is \", min_score, \" between words \", min_words)\n",
        "      #     else:\n",
        "      #         print(\"No min! Scores are: \", similarity_scores)\n",
        "    return similarity_scores_avg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gq1sXn69dV8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cut(obj, sec):\n",
        "    return [obj[i:i+sec] for i in range(0,len(obj),sec)]"
      ],
      "metadata": {
        "id": "6LBnOmRyiKlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_sent=furtherclean(sent)"
      ],
      "metadata": {
        "id": "8As_N7pniLO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = clean_sent\n",
        "vec_map = wordmap\n",
        "r = cut([i for i in range(84000)], 200)\n",
        "# weighted = True\n",
        "similarity_scores_avg = []\n",
        "for d in r:\n",
        "    for index in d:\n",
        "        avg_max_score = 0\n",
        "        avg_min_score = 0\n",
        "        sent_no = 200\n",
        "        max_s = []\n",
        "        min_s = []\n",
        "        tweet_tokens = tweet[index]\n",
        "        similarity_dic = dict.fromkeys([\"most similar\", \"most dissimilar\"], 0.0)\n",
        "        scores = []\n",
        "        max_score = -100\n",
        "        min_score = 100\n",
        "        for i in range(len(tweet_tokens)-1):\n",
        "            wi = tweet_tokens[i]        \n",
        "            for j in range(i + 1, len(tweet_tokens)):\n",
        "                wj = tweet_tokens[j]\n",
        "                # print(wi,wj)\n",
        "                if wi in vec_map.keys() and wj in vec_map.keys():\n",
        "\n",
        "                    similarity = cosine_similarity(vec_map[wi], vec_map[wj])\n",
        "                    # print(similarity)\n",
        "                    if max_score < similarity:\n",
        "                        max_score = similarity\n",
        "                        max_words = [wi, wj]\n",
        "                    if min_score > similarity:\n",
        "                        min_score = similarity\n",
        "                        min_words = [wi, wj]\n",
        "                else: continue\n",
        "\n",
        "  \n",
        "        # max_score_a=max(scores)\n",
        "        # min_score_a=min(scores)\n",
        "        max_s.append(max_score)\n",
        "        min_s.append(min_score)\n",
        "    max_avg = (sum(max_s))/sent_no\n",
        "    min_avg = (sum(min_s))/sent_no\n",
        "    similarity_dic['most similar']= max_avg\n",
        "    similarity_dic['most dissimilar']=min_avg\n",
        "    # similarity_dic['sentence number']=sent_no\n",
        "    similarity_scores_avg.append(similarity_dic)"
      ],
      "metadata": {
        "id": "MXAcePvpFIga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_train,sim_test,y_train,y_test = train_test_split(similarity_scores_avg,list(y),test_size = 0.2,shuffle = True,random_state=42)"
      ],
      "metadata": {
        "id": "WBRLxIEqG-Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pF-S--nFreYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lU9r9kpHra3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRwzmghtF_wc"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "def extract_features_from_dict(train_features, test_features):\n",
        "    # Transform the list of feature-value mappings to a vector\n",
        "    vector = DictVectorizer(sparse=False)\n",
        "    # Learn a list of feature name -> indices mappings and transform X_train_features\n",
        "    x_train_features = vector.fit_transform(train_features).tolist()\n",
        "    # Just transform the X_test_features, based on the list fitted on X_train_features\n",
        "    # Disadvantage: named features not encountered during fit_transform will be silently ignored.\n",
        "    x_test_features = vector.transform(test_features).tolist()\n",
        "    print('Size of the feature sets: train =  ', len(x_train_features[0]), ', test = ', len(x_test_features[0]))\n",
        "    return np.nan_to_num(np.array(x_train_features)), np.nan_to_num(np.array(x_test_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o1i5H3QI5sG"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFi7qm6MUOJQ"
      },
      "outputs": [],
      "source": [
        "x_train_normal_feature=normal_feature(x_train)\n",
        "x_test_normal_feature=normal_feature(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkG6LEboUZJr"
      },
      "outputs": [],
      "source": [
        "x_train_senti_feature=senti_feature(x_train,subj_dict,emojii)\n",
        "x_test_senti_feature=senti_feature(x_test,subj_dict,emojii)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_sim,x_test_sim=extract_features_from_dict(sim_train,sim_test)"
      ],
      "metadata": {
        "id": "iYNDIHfe_8IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vxvpgddUYM9"
      },
      "outputs": [],
      "source": [
        "def merge_dicts(*dict_args):\n",
        "    result = {}\n",
        "    for dictionary in dict_args:\n",
        "        result.update(dictionary)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwdxS3RmgL4g"
      },
      "outputs": [],
      "source": [
        "\n",
        "x_train_normal_feature, x_test_normal_feature=extract_features_from_dict(x_train_normal_feature, x_test_normal_feature)\n",
        "x_train_senti_feature, x_test_senti_feature=extract_features_from_dict(x_train_senti_feature, x_test_senti_feature)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovFKh6l4Svho"
      },
      "outputs": [],
      "source": [
        "# x_train_f = np.hstack((x_train_glove,x_train_feature,tf_word_train,posfeature_train))#tf_word_train,\n",
        "# x_test_f = np.hstack((x_test_glove,x_test_feature,tf_word_test,posfeature_test))#,tf_char_test\n",
        "x_train_f = np.hstack((x_train_glove,x_train_normal_feature,x_train_senti_feature,tf_word_train,posfeature_train,x_train_sim))#tf_word_train,\n",
        "x_test_f = np.hstack((x_test_glove,x_test_normal_feature,x_test_senti_feature,tf_word_test,posfeature_test,x_test_sim))#,tf_char_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02IPTk6xcY_q"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "model_name = \"bert-large-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "def bert_encoder(review):\n",
        "    encoded = tokenizer(review, truncation=True, max_length=20000, pad_to_max_length=True)\n",
        "    return encoded['input_ids'], encoded['token_type_ids'], encoded['attention_mask']\n",
        "\n",
        "x_train_bert = [bert_encoder(r) for r in x_train]\n",
        "x_train_bert = np.array(x_train_bert)\n",
        "x_test_bert = [bert_encoder(r) for r in x_test]\n",
        "x_test_bert = np.array(x_test_bert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcn0oDGvoDvQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "#Classifiers\n",
        "from sklearn import svm\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "#Validation packages\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import cross_val_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS-rAa-Fii9d"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "# scaler = preprocessing.StandardScaler().fit(x_train_feature)\n",
        "# x_train_feature_n=scaler.transform(x_train_feature)\n",
        "# x_test_feature_n=scaler.transform(x_test_feature)\n",
        "\n",
        "scaler_normal = preprocessing.StandardScaler().fit(x_train_normal_feature)\n",
        "x_train_normal_feature_n=scaler_normal.transform(x_train_normal_feature)\n",
        "x_test_normal_feature_n=scaler_normal.transform(x_test_normal_feature)\n",
        "\n",
        "scaler_senti = preprocessing.StandardScaler().fit(x_train_senti_feature)\n",
        "x_train_senti_feature_n=scaler_senti.transform(x_train_senti_feature)\n",
        "x_test_senti_feature_n=scaler_senti.transform(x_test_senti_feature)\n",
        "\n",
        "scaler_sim = preprocessing.StandardScaler().fit(x_train_sim)\n",
        "x_train_sim_n=scaler_sim.transform(x_train_sim)\n",
        "x_test_sim_n=scaler_sim.transform(x_test_sim)\n",
        "\n",
        "x_train_f = np.hstack((x_train_glove,x_train_normal_feature,x_train_senti_feature,tf_word_train,posfeature_train,x_train_sim))#tf_word_train,\n",
        "x_test_f = np.hstack((x_test_glove,x_test_normal_feature,x_test_senti_feature,tf_word_test,posfeature_test,x_test_sim))#,tf_char_test\n",
        "scaler2 = preprocessing.StandardScaler().fit(x_train_f)\n",
        "x_train_fn = scaler2.transform(x_train_f)\n",
        "x_test_fn = scaler2.transform(x_test_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIZ7Rvb3j05H"
      },
      "outputs": [],
      "source": [
        "print(x_test_bert.shape)\n",
        "x_train_re = x_train_bert.reshape((336,3*20000))\n",
        "x_test_re = x_test_bert.reshape((84,3*20000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NoPEpFXhh3q"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sv(x_train_fn, y_train,x_test_fn, y_test):\n",
        "  clf_svm = svm.SVC()\n",
        "  clf_svm.fit(x_train_fn, y_train)\n",
        "  y_pred_svmall=clf_svm.predict(x_test_fn)\n",
        "  print('acc {}'.format(accuracy_score(y_test,y_pred_svmall)))\n",
        "  print('f1_score {}'.format(f1_score(y_test,y_pred_svmall)))\n",
        "  return accuracy_score(y_test,y_pred_svmall), f1_score(y_test,y_pred_svmall)"
      ],
      "metadata": {
        "id": "fmFyXovOCoBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sv(posfeature_train, y_train,posfeature_test,y_test)\n",
        "sv(tf_word_train, y_train,tf_word_test,y_test)\n",
        "sv(np.hstack([x_train_glove,x_train_sim]), y_train,np.hstack([x_test_glove,x_test_sim]),y_test)"
      ],
      "metadata": {
        "id": "GbBeUATFKti5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sv(x_train_normal_feature, y_train,x_test_normal_feature,y_test)\n",
        "sv(x_train_senti_feature, y_train,x_test_senti_feature,y_test)\n",
        "sv(x_train_fn, y_train,x_test_fn,y_test)"
      ],
      "metadata": {
        "id": "Pg7Cb4haK1Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LR(x_train_fn, y_train,x_test_fn, y_test):\n",
        "  lr_all=LogisticRegression(solver=\"liblinear\",random_state=2)\n",
        "  lr_all.fit(x_train_fn, y_train)\n",
        "  y_pred_lrall=lr_all.predict(x_test_fn)\n",
        "  print('acc {}'.format(accuracy_score(y_test,y_pred_lrall)))\n",
        "  print('f1_score {}'.format(f1_score(y_test,y_pred_lrall)))\n",
        "  return accuracy_score(y_test,y_pred_lrall), f1_score(y_test,y_pred_lrall)"
      ],
      "metadata": {
        "id": "1-jfnqGhvw0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR(posfeature_train, y_train,posfeature_test,y_test)"
      ],
      "metadata": {
        "id": "UwTwNqRBwLcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR(tf_word_train, y_train,tf_word_test,y_test)"
      ],
      "metadata": {
        "id": "lr7md0laIto5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR(np.hstack([x_train_glove,x_train_sim]), y_train,np.hstack([x_test_glove,x_test_sim]),y_test)"
      ],
      "metadata": {
        "id": "AAEvry4Kwa0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR(x_train_normal_feature, y_train,x_test_normal_feature,y_test)\n",
        "LR(x_train_senti_feature, y_train,x_test_senti_feature,y_test)\n",
        "LR(x_train_fn, y_train,x_test_fn,y_test)"
      ],
      "metadata": {
        "id": "fzEYKmuIJMAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__GULaHFlUbm"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "def RF(x_train_fn, y_train,x_test_fn,y_test):\n",
        "  clf_all = RandomForestClassifier(max_depth=8, random_state=0)\n",
        "  clf_all.fit(x_train_fn, y_train)\n",
        "  y_pred_all=clf_all.predict(x_test_fn)\n",
        "  print('acc {}'.format(accuracy_score(y_test,y_pred_all)))\n",
        "  print('f1_score {}'.format(f1_score(y_test,y_pred_all)))\n",
        "  return accuracy_score(y_test,y_pred_all), f1_score(y_test,y_pred_all)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RF(posfeature_train, y_train,posfeature_test,y_test)"
      ],
      "metadata": {
        "id": "yA7NNoL3BtHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF(tf_word_train, y_train,tf_word_test,y_test)"
      ],
      "metadata": {
        "id": "avikMRD7E6qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF(np.hstack([x_train_glove,x_train_sim]), y_train,np.hstack([x_test_glove,x_test_sim]),y_test)"
      ],
      "metadata": {
        "id": "U6za1wmOGEOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF(x_train_normal_feature, y_train,x_test_normal_feature,y_test)"
      ],
      "metadata": {
        "id": "VbR_ynjYHM7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF(x_train_senti_feature, y_train,x_test_senti_feature,y_test)"
      ],
      "metadata": {
        "id": "ongAglmkGL24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF(x_train_fn, y_train,x_test_fn,y_test)"
      ],
      "metadata": {
        "id": "A_rLZvw-HlBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToOiApiH5qJQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
        "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPooling1D\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM,Conv2D,Conv1D\n",
        "# from keras.layers import Merge\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "# from theano.tensor.shared_randomstreams import RandomStreams\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Reshape\n",
        "from keras import optimizers\n",
        "from keras.regularizers import l2\n",
        "from keras.regularizers import l1\n",
        "# from keras.layers.normalization import BatchNormalization\n",
        "from keras.constraints import nonneg\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from tensorflow.keras.optimizers import SGD, Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TiZZZnBPQ_Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcknU47A__KG"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "# scaler = preprocessing.StandardScaler().fit(x_train_feature)\n",
        "# x_train_feature_n=scaler.transform(x_train_feature)\n",
        "# x_test_feature_n=scaler.transform(x_test_feature)\n",
        "\n",
        "scaler_normal = preprocessing.StandardScaler().fit(x_train_normal_feature)\n",
        "x_train_normal_feature_n=scaler_normal.transform(x_train_normal_feature)\n",
        "x_test_normal_feature_n=scaler_normal.transform(x_test_normal_feature)\n",
        "\n",
        "scaler_senti = preprocessing.StandardScaler().fit(x_train_senti_feature)\n",
        "x_train_senti_feature_n=scaler_senti.transform(x_train_senti_feature)\n",
        "x_test_senti_feature_n=scaler_senti.transform(x_test_senti_feature)\n",
        "\n",
        "scaler_pos = preprocessing.StandardScaler().fit(posfeature_train)\n",
        "x_train_pos_feature_n=scaler_pos.transform(posfeature_train)\n",
        "x_test_pos_feature_n=scaler_pos.transform(posfeature_test)\n",
        "\n",
        "scaler_tf = preprocessing.StandardScaler().fit(tf_word_train)\n",
        "x_train_tf_feature_n=scaler_tf.transform(tf_word_train)\n",
        "x_test_tf_feature_n=scaler_tf.transform(tf_word_test)\n",
        "\n",
        "scaler_sim = preprocessing.StandardScaler().fit(x_train_sim)\n",
        "x_train_sim_n=scaler_sim.transform(x_train_sim)\n",
        "x_test_sim_n=scaler_sim.transform(x_test_sim)\n",
        "\n",
        "# x_train_f = np.hstack((x_train_glove,x_train_feature,tf_word_train,posfeature_train))\n",
        "# x_test_f = np.hstack((x_test_glove,x_test_feature,tf_word_test,posfeature_test))\n",
        "x_train_f = np.hstack((x_train_glove,x_train_normal_feature,x_train_senti_feature,tf_word_train,posfeature_train,x_train_sim))#tf_word_train,\n",
        "x_test_f = np.hstack((x_test_glove,x_test_normal_feature,x_test_senti_feature,tf_word_test,posfeature_test,x_test_sim))#,tf_char_test\n",
        "scaler_full = preprocessing.StandardScaler().fit(x_train_f)\n",
        "x_train_full_n=scaler_full.transform(x_train_f)\n",
        "x_test_full_n=scaler_full.transform(x_test_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVdInlQkAOSq"
      },
      "outputs": [],
      "source": [
        "x0=np.array(x_train_glove)[:,np.newaxis,:]\n",
        "x1=x_train_normal_feature_n[:,np.newaxis,:]\n",
        "x2=x_train_senti_feature_n[:,np.newaxis,:]\n",
        "x3 = x_train_pos_feature_n[:,np.newaxis,:]\n",
        "x4 = x_train_tf_feature_n[:,np.newaxis,:]\n",
        "x5=x_train_sim_n[:,np.newaxis,:]\n",
        "x0_test=x_test_glove[:,np.newaxis,:]\n",
        "x1_test=x_test_normal_feature_n[:,np.newaxis,:]\n",
        "x2_test=x_test_senti_feature_n[:,np.newaxis,:]\n",
        "x3_test = x_test_pos_feature_n[:,np.newaxis,:]\n",
        "x4_test = x_test_tf_feature_n[:,np.newaxis,:]\n",
        "x5_test=x_test_sim_n[:,np.newaxis,:]\n",
        "y_train_1=y_train[:,np.newaxis]\n",
        "y_test_1=y_test[:,np.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model_fusion(exits, num_aux_targets):\n",
        "    # ipt1=input[0]\n",
        "    # ipt2=input[1]\n",
        "    # ipt3=input[2]\n",
        "    # ipt4=input[3]\n",
        "    # ipt5=input[4]\n",
        "\n",
        "    ipt0 = Input((1,50))\n",
        "    # bp0 = LSTM(128)(ipt0)\n",
        "    # bp0 = LSTM(128)(ipt0)\n",
        "    # bp0=Dropout(0.4)(bp0)\n",
        "    # x=layers.Conv1D(128,64)()\n",
        "    # x=layers.MaxPooling1D()(x)\n",
        "    # x=Dropout(0.5)(x)                 \n",
        "    # x=GlobalAveragePooling1D()(x)\n",
        "    bp0 = Dense(64, activation='relu')(ipt0)\n",
        "    bp0=Dropout(0.5)(bp0)\n",
        "    d0 = Dense(64, activation='relu')(bp0)\n",
        "\n",
        "    ipt1 = Input((1,6))\n",
        "    # x = LSTM(10)(ipt1)\n",
        "    # x=Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(32, activation='relu')(ipt1)\n",
        "    x= Dropout(0.5)(x)\n",
        "    d1 = Dense(8, activation='relu')(x)\n",
        "\n",
        "    ipt2=Input((1,16))\n",
        "    # bp2=LSTM(32)(ipt2)\n",
        "    # bp2=Dropout(0.3)(bp2)\n",
        "    bp2=Dense(32,activation=\"relu\")(ipt2)\n",
        "    bp2=Dropout(0.5)(bp2)\n",
        "    d2=Dense(16,activation=\"relu\")(bp2)\n",
        "\n",
        "    ipt3=Input((1,30))\n",
        "    # bp3=LSTM(32)(ipt3)\n",
        "    # bp3=Dropout(0.3)(bp3)\n",
        "    bp3=Dense(64,activation=\"relu\")(ipt3)\n",
        "    bp3=Dropout(0.5)(bp3)\n",
        "    d3=Dense(32,activation=\"relu\")(bp3)\n",
        "\n",
        "    ipt4=Input((1,32))\n",
        "    # bp4=LSTM(64)(ipt4)\n",
        "    # bp4=Dropout(0.3)(bp4)\n",
        "    bp4=Dense(32,activation=\"relu\")(ipt4)\n",
        "    bp4=Dropout(0.5)(bp4)\n",
        "    d4=Dense(32,activation=\"relu\")(bp4)\n",
        "\n",
        "    ipt5=Input((1,2))\n",
        "    # bp5=LSTM(32)(ipt5)\n",
        "    # bp5=Dropout(0.3)(bp5)\n",
        "    bp5=Dense(32,activation=\"relu\")(ipt5)\n",
        "    bp5=Dropout(0.5)(bp5)\n",
        "    d5=Dense(16,activation=\"relu\")(bp5)\n",
        "\n",
        "    a=4\n",
        "    b=8\n",
        "    if exits==[0]:\n",
        "      m=tf.keras.layers.Concatenate(axis=-1)([d0,d5])\n",
        "      # m=d\n",
        "    if exits==[1]:\n",
        "      m=d1\n",
        "    if exits==[2]:\n",
        "      m=d2\n",
        "    if exits==[3]:\n",
        "      m=d3\n",
        "    if exits==[4]:\n",
        "      m=d4\n",
        "    if exits==[0,1,2,3,4]:\n",
        "      m=tf.keras.layers.Concatenate(axis=-1)([d0,d1,d2,d3,d4,d5])\n",
        "    # print(m.shape)\n",
        "    merged=Dropout(0.4)(m)\n",
        "    merged=Dense(128,activation='relu', kernel_regularizer=l2(0.02), name = 'fusion_layer_1')(merged)\n",
        "    merged=Dropout(0.4)(merged)\n",
        "    merged=Dense(128, activation='relu', kernel_regularizer=l2(0.01), name = 'fusion_layer_2')(merged)\n",
        "    # merged += m\n",
        "    merged=Dropout(0.5)(merged)\n",
        "    result=Dense(1, activation='sigmoid',kernel_regularizer=l2(0.005))(merged)\n",
        "    # result=Dense(1, weights=[np.array([[b]]),np.array([-a])], trainable=False)\n",
        "    # aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
        "    \n",
        "    train_patience=5\n",
        "    train_epoch=10\n",
        "    weights_path=\"/content/drive/MyDrive/nlp\"\n",
        "    callbacks = [\n",
        "      EarlyStopping(monitor='val_loss', patience=train_patience, verbose=0),\n",
        "      ModelCheckpoint(weights_path, monitor='val_loss', save_best_only=True, verbose=0),\n",
        "    ]\n",
        "    adm = keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "    sgd = SGD(learning_rate=0.04, decay=1e-6, momentum=0.1, nesterov=True)\n",
        "\n",
        "    fusion_model = Model(inputs=[ipt0,ipt1,ipt2,ipt3,ipt4,ipt5], outputs=result)\n",
        "    fusion_model.compile(loss='binary_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "    fusion_model.run_eagerly = True\n",
        "\n",
        "    return fusion_model"
      ],
      "metadata": {
        "id": "eRX0iVbpItnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "checkpoint_predictions_1 = []\n",
        "weights_1 = []\n",
        "y_num=2\n",
        "NUM_MODELS=1\n",
        "EPOCHS=25\n",
        "best_acc=0\n",
        "loss_train1=[]\n",
        "acc_train1=[]\n",
        "loss_test1=[]\n",
        "acc_test1=[]\n",
        "for model_idx in range(NUM_MODELS):\n",
        "    model = build_model_fusion([0,1,2,3,4],y_train.shape[0])\n",
        "    gg=0\n",
        "    for global_epoch in range(EPOCHS):\n",
        "        if (global_epoch%20) ==0:\n",
        "          gg+=1\n",
        "        history=model.fit(\n",
        "            [x0,x1,x2,x3,x4,x5],\n",
        "            y_train_1,\n",
        "            validation_data=([x0_test,x1_test,x2_test,x3_test,x4_test,x5_test],y_test_1),\n",
        "            batch_size=8,\n",
        "            epochs=1,\n",
        "            verbose=2\n",
        "\n",
        "            # callbacks=[\n",
        "            #     EarlyStopping(monitor='val_loss', patience=5, verbose=0),\n",
        "            #     LearningRateScheduler(lambda epoch: 0.05-(0.005 * (gg-1)))\n",
        "            # ]\n",
        "        )\n",
        "        if best_acc < history.history['val_accuracy'][0]:\n",
        "          best_acc=history.history['val_accuracy'][0]\n",
        "          model.save('/content/drive/MyDrive/nlp/my_h5_model.h5')\n",
        "\n",
        "        loss_train1.append(history.history['loss'])\n",
        "        acc_train1.append(history.history['accuracy'])\n",
        "        loss_test1.append(history.history['val_loss'])\n",
        "        acc_test1.append(history.history['val_accuracy'])\n",
        "        checkpoint_predictions_1.append(model.predict([x0_test,x1_test,x2_test,x3_test,x4_test,x5_test], batch_size=8))\n",
        "        weights_1.append(2 ** global_epoch)\n",
        "\n",
        "predictions = np.average(checkpoint_predictions_1, weights=weights_1, axis=0)\n",
        "model=keras.models.load_model('/content/drive/MyDrive/nlp/my_h5_model.h5')\n",
        "pp=model.predict([x0_test,x1_test,x2_test,x3_test,x4_test,x5_test])\n",
        "ppp=pp.copy()\n",
        "ppp[pp<0.5]=0\n",
        "ppp[pp>=0.5]=1\n",
        "ppp=np.squeeze(ppp)\n",
        "print(\"acc {}\".format(accuracy_score(ppp, y_test)))\n",
        "print(\"f1 {}\".format(f1_score(ppp, y_test)))\n",
        "# model.fit([x0,x1,x2,x3,x4,x5],y_train,validation_data=([x0_test,x1_test,x2_test,x3_test,x4_test,x5_test],y_test),epochs=20,batch_size=8)\n",
        "\n",
        "plt.figure(figsize=(18,6))\n",
        "plt.subplot(121)\n",
        "plt.plot(loss_train1)\n",
        "plt.plot(loss_test1)\n",
        "plt.title('Feature-level Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-W-65pIg6lZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit(xxx,y_train,validation_data=(xxxx,y_test),epochs=10,batch_size=8)\n",
        "def model2(xx_test, xx_train, y_test, y_train,path):\n",
        "  model = tf.keras.Sequential([ #build the Bi-LSTM model\n",
        "      # tf.keras.layers.Embedding(1,136),\n",
        "      # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,return_sequences=True)),\n",
        "      # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
        "      tf.keras.layers.Dense(256,activation='relu',kernel_regularizer=l2(0.01)),\n",
        "      tf.keras.layers.Dropout(0.5),\n",
        "      tf.keras.layers.Dense(128,activation='relu',kernel_regularizer=l2(0.01)),\n",
        "      tf.keras.layers.Dropout(0.5),\n",
        "      # tf.keras.layers.Dense(128,activation='relu',kernel_regularizer=l2(0.01)),\n",
        "      # tf.keras.layers.Dropout(0.5),\n",
        "      tf.keras.layers.Dense(64,activation='relu',kernel_regularizer=l2(0.007)),\n",
        "      tf.keras.layers.Dropout(0.5),\n",
        "      tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "  ])\n",
        "  adm = keras.optimizers.Adam(learning_rate=0.01, epsilon=1e-07)#use the momentum sgd\n",
        "  sgd = SGD(learning_rate=0.02, decay=1e-6, momentum=0.1, nesterov=True)\n",
        "  model.compile(loss='binary_crossentropy',optimizer=adm, metrics=['accuracy'])#compile the model\n",
        "  model.run_eagerly = True\n",
        "\n",
        "\n",
        "  checkpoint_predictions = []\n",
        "  weights = []\n",
        "  y_num=2\n",
        "  NUM_MODELS = 1\n",
        "  BATCH_SIZE = 512\n",
        "  LSTM_UNITS = 128\n",
        "  DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "  EPOCHS = 10\n",
        "  MAX_LEN = 22\n",
        "  best_acc=0\n",
        "  loss_train=[]\n",
        "  acc_train=[]\n",
        "  loss_test=[]\n",
        "  acc_test=[]\n",
        "  for model_idx in range(NUM_MODELS):\n",
        "      for global_epoch in range(EPOCHS):\n",
        "        history=model.fit(\n",
        "              xx_train,\n",
        "              y_train,\n",
        "              batch_size=8,\n",
        "              epochs=1,\n",
        "              verbose=2,\n",
        "              validation_data=(xx_test,y_test),\n",
        "              callbacks=[\n",
        "                  LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
        "              ]\n",
        "          )\n",
        "        if best_acc < history.history['val_accuracy'][0]:\n",
        "          best_acc=history.history['val_accuracy'][0]\n",
        "          model.save('/content/drive/MyDrive/nlp/my_h5_model2.h5')\n",
        "\n",
        "        loss_train.append(history.history['loss'])\n",
        "        acc_train.append(history.history['accuracy'])\n",
        "        loss_test.append(history.history['val_loss'])\n",
        "        acc_test.append(history.history['val_accuracy'])\n",
        "\n",
        "        checkpoint_predictions.append(model.predict(xx_test, batch_size=8))\n",
        "        weights.append(2 ** global_epoch)\n",
        "\n",
        "  model=keras.models.load_model('/content/drive/MyDrive/nlp/my_h5_model2.h5')\n",
        "  predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n",
        "  pp=model.predict(xx_test)\n",
        "  ppp=pp.copy()\n",
        "  ppp[pp<0.5]=0\n",
        "  ppp[pp>=0.5]=1\n",
        "  ppp=np.squeeze(ppp)\n",
        "  print(\"acc {}\".format(accuracy_score(ppp, y_test)))\n",
        "  print(\"f1 {}\".format(f1_score(ppp, y_test)))\n",
        "  # model.fit(x_test_fn,y_train,validation_data=(x_test_fn,y_test),epochs=10,batch_size=8)\n",
        "  plt.figure(figsize=(18,6))\n",
        "  plt.subplot(121)\n",
        "  plt.plot(loss_train)\n",
        "  plt.plot(loss_test)\n",
        "  plt.title('Model-level Loss',fontsize=15)\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "30pE1S2sbx26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2(x_test_fn, x_train_fn, y_test, y_train,'/content/drive/MyDrive/nlp/model2.jpg')"
      ],
      "metadata": {
        "id": "7yUQJz0hoy-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COLS=[]\n",
        "nomal=['word_length', 'woken_length', 'avg_token_length', 'capitalized', 'specific', 'intensifier']\n",
        "sen=['Emoji_positive', 'Emoji_negative', 'Emoji_neutral', 'subjective_weak', 'subjective_strong', 'subjective_positive', 'subjective_negative', 'subjlexicon_neutral', 'subjective_total', 'swn_positive', 'swn_negative', 'swn_compound', 'vad_positive', 'vad_negative', 'vad_neutral', 'vad_compound']\n",
        "\n",
        "for i in range(50):\n",
        "  COLS.append('glove'+str(i+1))\n",
        "COLS=COLS+nomal+sen\n",
        "for i in range(72,102):\n",
        "  COLS.append('pos'+str(i-71))\n",
        "for i in range(102,134):    \n",
        "  COLS.append('tfidf'+str(i-101))\n",
        "COLS.append('max_similar')\n",
        "COLS.append('min_similar')"
      ],
      "metadata": {
        "id": "ALbfegqCh9E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_fffff=x_train_full_n[:,np.newaxis,:]\n",
        "x_test_fffff=x_test_full_n[:,np.newaxis,:]"
      ],
      "metadata": {
        "id": "KF3QBYrIPtrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TYPE=['Semantic']*50\n",
        "TYPE=TYPE+['Normal']*6+['Sentiment']*16+['Syntactic']*30+['Lexical']*32+['Semantic']*2"
      ],
      "metadata": {
        "id": "LaNFZJ0wRlVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        " \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from sklearn.preprocessing import RobustScaler, normalize\n",
        "from sklearn.model_selection import train_test_split, GroupKFold, KFold\n",
        "from IPython.display import display\n",
        " \n",
        "COMPUTE_LSTM_IMPORTANCE = 1\n",
        "ONE_FOLD_ONLY = 1\n",
        "NUM_FOLDS=5\n",
        " \n",
        "# with gpu_strategy.scope():\n",
        "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n",
        "test_preds = []\n",
        "results_all=[]\n",
        "for fold, (train_idx, test_idx) in enumerate(kf.split(x_train_full_n, y_train)):\n",
        "    K.clear_session()\n",
        "    \n",
        "    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n",
        "    X_train, X_valid = x_fffff[train_idx], x_fffff[test_idx]\n",
        "    Y_train1, y_valid = y_train[train_idx], y_train[test_idx]\n",
        "\n",
        "# for i in range(50):\n",
        "#   COLS.append('glove'+str(i+1))\n",
        "# COLS=COLS+nomal+sen\n",
        "# for i in range(72,102):\n",
        "#   COLS.append('pos'+str(i-71))\n",
        "# for i in range(102,134):    \n",
        "#   COLS.append('tfidf'+str(i-101))\n",
        "# COLS.append('max_similar')\n",
        "# COLS.append('min_similar')   \n",
        "    # 导入已经训练好的模型\n",
        "    model = keras.models.load_model('/content/drive/MyDrive/nlp/my_h5_model-1.h5')\n",
        "    # 计算特征重要性\n",
        "    if COMPUTE_LSTM_IMPORTANCE:\n",
        "        results = []\n",
        "        print(' Computing LSTM feature importance...')\n",
        "\n",
        "        for k in tqdm(range(len(COLS))):\n",
        "            # if k>0: \n",
        "            save_col = X_valid[:,:,k].copy()\n",
        "            np.random.shuffle(X_valid[:,:,k])\n",
        "            # save_col = X_valid[:,:,k-1].copy()\n",
        "            # np.random.shuffle(X_valid[:,:,k-1])\n",
        "            x0_f,x1_f,x2_f,x3_f,x4_f,x5_f=X_valid[:,:,:50],X_valid[:,:,50:56],X_valid[:,:,56:72],X_valid[:,:,72:102],X_valid[:,:,102:134],X_valid[:,:,134:]\n",
        "            oof_preds = model.predict([x0_f,x1_f,x2_f,x3_f,x4_f,x5_f], verbose=0).squeeze() \n",
        "            loss=np.mean(y_valid*np.log(oof_preds)+(1-y_valid)*np.log(1-oof_preds))\n",
        "            # mse = np.mean(( oof_preds-y_valid )**2)\n",
        "            # results.append({'feature':COLS[k],'loss':loss})  \n",
        "            results.append(loss)\n",
        "            if k>0: \n",
        "                X_valid[:,:,k-1] = save_col\n",
        "        results_all.append(results)\n",
        "results=np.mean(results_all,axis=0)\n"
      ],
      "metadata": {
        "id": "iHQygqlKUFkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df={'features':COLS,'loss':-results}\n",
        "df=pd.DataFrame(df)\n",
        "df = df.sort_values('loss')\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.barh(np.arange(len(COLS)),df.loss)\n",
        "plt.yticks(np.arange(len(COLS)),df.features.values)\n",
        "plt.title('BiLSTM Feature Importance',size=16)\n",
        "plt.ylim((-1,len(COLS)))\n",
        "plt.show()\n",
        "                    \n",
        "# SAVE LSTM FEATURE IMPORTANCE\n",
        "# df = df.sort_values('loss',ascending=False)\n",
        "# df.to_csv(f'bilstm_feature_importance_fold_{fold}.csv',index=False)"
      ],
      "metadata": {
        "id": "Q41pm_WYSPeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TYPE=['Semantic']*50\n",
        "TYPE=TYPE+['Normal']*6+['Sentiment']*16+['Syntactic']*30+['Lexical']*32+['Semantic']*2\n",
        "\n",
        "df={'type':TYPE,'features':COLS,'loss':results}\n",
        "colors={'Normal':'#CD5C5C','Sentiment':'#00BFFF','Syntactic':'orange','Semantic':'#696969','Lexical':'#32CD32'}\n",
        "df=pd.DataFrame(df)\n",
        "# df = df.sort_values('loss')\n",
        "fea_more=nomal+sen+['tfidf','glove','pos']+['max_similar','min_similar']\n",
        "cate=['Normal']*6+['Sentiment']*16+['Lexical','Semantic','Syntactic']+['Semantic']*2\n",
        "loss_avg=[]\n",
        "for x in fea_more:\n",
        "  bool_index = df.features.str.contains(x)\n",
        "  l=np.mean(df[bool_index]['loss'])\n",
        "  loss_avg.append(l)\n",
        "s=np.flipud(np.argsort(loss_avg))\n",
        "fea_more=np.array(fea_more)[s]\n",
        "cate=np.array(cate)[s]\n",
        "loss_avg=np.array(loss_avg)\n",
        "loss_avg=abs(np.sort(-loss_avg))\n",
        "plt.figure(figsize=(12,10))\n",
        "\n",
        "# abels = df['Condition'].unique()\n",
        "\n",
        "\n",
        "plt.barh(np.arange(len(fea_more)),loss_avg,height=0.5, color=[colors[i] for i in cate])\n",
        "plt.yticks(np.arange(len(fea_more)),fea_more)\n",
        "plt.title('Permutation Feature Importance',size=15)\n",
        "plt.ylabel('features',size=10)\n",
        "plt.xlabel('absolute binary crossentropy loss',size=15)\n",
        "plt.ylim((-1,len(fea_more)))\n",
        "plt.xlim((min(loss_avg)-0.4,1.15))\n",
        "cate_u=list(set(cate))\n",
        "handles = [plt.Rectangle((0,0),1,1, color=colors[l]) for l in cate_u]\n",
        "plt.legend(handles, cate_u)\n",
        "plt.savefig('importance.jpg',dpi=600)\n",
        "plt.show()\n",
        "# bool_index1 = df.features.str.contains('tfidf')\n",
        "# bool_index2 = df.features.str.contains('senti')\n",
        "# bool_index3 = df.features.str.contains('glove')\n",
        "# bool_index4 = df.features.str.contains('pos')\n",
        "# bool_index5 = df.features.str.contains('normal')\n",
        "# loss_avg.append(bool_index)"
      ],
      "metadata": {
        "id": "cEbOqx7NLUDZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "lp5.ipynb（副本）（副本）",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}